#!/bin/bash
# Copyright (c) 2016 EMC Corporation
# All Rights Reserved
#
# This software contains the intellectual property of EMC Corporation
# or is licensed to EMC Corporation from third parties.  Use of this
# software and the intellectual property contained therein is expressly
# limited to the terms and conditions of the License Agreement under which
# it is provided by or on behalf of EMC.
#

DIAGCOLLECT_DIR="/tmp/diagutils"
DIAGCOLLECT_MAME="diagutils-`date +%Y%0m%0d%0k%0M%0S`"
DIAGCOLLECT_ARCHIVE_NAME="${DIAGCOLLECT_MAME}"
DIAGCOLLECT_OUTPUT="${DIAGCOLLECT_DIR}/${DIAGCOLLECT_MAME}"
COMMAND_DIR="/opt/storageos/bin"
MIN_CFS=(BlockConsistencyGroup BlockMirror BlockSnapshot Cluster ExportGroup ExportMask FCZoneReference Host Initiator Network NetworkSystem ProtectionSet ProtectionSystem StoragePort StorageSystem VirtualArray VirtualDataCenter VirtualPool Volume)

#This script is used to help user to restore a backupset simply
usage() {
    echo "Usage:"
    echo "       $0 <-all|-quick>|<-min_cfs|-all_cfs|-zk|-backup|-logs|-properties|-health> [-ftp <server name or ip and directory> -u <user name> -p <password>]"
    echo "Options:"
    echo "       -min_cfs               Collect column families through output of dbutils list and/or cqlsh"
    echo "                              minimal column families include(could be reconfigured by editing $0):"
    echo "                              VirtualPool, VirtualDataCenter, Volume, ExportMask, ExportGroup, Host, Cluster, StoragePort, StorageSystem, Initiator, FCZoneReference, Network,"
    echo "                              NetworkSystem, Vcenter, ProtectionSet, BlockConsistencyGroup, VirtualArray, StoragePool, and RelationIndex and AltIndex."
    echo "       -all_cfs               Collect all column families through output of dbutils list and/or cqlsh."
    echo "       -zk                    Collect zk jobs and queues through zkutils."
    echo "       -backup [backup name]  Create a new ViPR system backup/dump of DB and ZK through bkutils, which can be restored later."
    echo "                              If the backup name is not specified, timestamp will be used instead."
    echo "                              If the backup name already exists, the utility won't create it and just copy it into the archive."
    echo "       -logs                  Collect all system logs (/var/log/messages) and ViPR logs including the rotated ones."
    echo "       -properties            Collect system properties (version, node count, node names, etc.)."
    echo "       -health                Collect system health information (e.g. node and service status, etc.), performance data of local node from top output."
    echo "       -all                   Including the output gathered by options: backup (with default backup name), zk, logs, properties, and health."
    echo "       -quick                 Including the output gathered by options: minimal column families, zk, logs, properties and health."
    echo "       -ftp <server name or ip and directory> -u <user name> -p <password>"
    echo "                              If specified, the output will be transferred to the external ftp site"
    echo "                              Note: It's suggested to always dump the output to the FTP to retain space in ViPR nodes and not run into issue due to /tmp folder being full"
    echo "For example:"
    echo "       $0 -all -ftp ftp://10.247.101.11:/tmp -u usera -p xxx"
    echo "       $0 -quick"
    echo "       $0 -min_cfs -zk -logs"
    echo "Notes:"
    echo "       '-all' equal to '-backup -zk -logs -properties -health'"
    echo "       '-quick' equal to '-min_cfs -zk -logs -properties -health'" 
}

######################
# Collect Functions
######################
collect_cfs() {
    local option=$1

    local param="db-cfs"
    check_if_skip ${param}
    if [[ ${SKIP_COLLECT} == true ]] ; then
        return
    fi

    echo "Collecting cfs info.."
    local cfsDir="${DIAGCOLLECT_OUTPUT}/${param}"
    mkdir -p ${cfsDir}

    set +e
    diagnose_cfs ${cfsDir} "${option}"
    set -e
}

collect_zk() {
    local param="zk-info"
    check_if_skip ${param}
    if [[ ${SKIP_COLLECT} == true ]] ; then
        return
    fi

    echo "Collecting zk info.."
    local zkDir="${DIAGCOLLECT_OUTPUT}/$param"
    mkdir -p ${zkDir}

    set +e
    ${COMMAND_DIR}/zkutils path / > $zkDir/zk-path
    ${COMMAND_DIR}/zkutils ephemeral -withdata > $zkDir/zk-ephemeral
    set -e
}

collect_backup() {
    local backupName="${DIAGCOLLECT_MAME}"
    if [[ $# -eq 1 ]] && [[ $1 != -* ]]; then
        backupName=$1
    fi

    set +e
    check_if_skip "new backup"
    if [[ ${SKIP_COLLECT} == false ]] ; then
        echo "Collecting backup data.."
        local foundName=`${COMMAND_DIR}/bkutils -l | grep "${backupName}" | awk '{print $1}'`
        if [ "${foundName}" == "${backupName}" ]; then
            echo -e "\tBackup($backupName) exists"
        else
            echo -e -n "\tStarting to creat backup(${backupName}).."
            local task="${COMMAND_DIR}/bkutils -c ${backupName} -f"
            execute_with_progress_point "${task}" "true"
        fi
    fi

    echo -e -n "\tTrying to collect and archive existing backup data.."
    local backupDir="${DIAGCOLLECT_OUTPUT}/backup"
    mkdir -p ${backupDir}
    collect_data "/data/backup/${backupName}" "${backupDir}" "false"
  
    cd ${backupDir}
    local task="zip ${backupName}.zip *"
    execute_with_progress_point "${task}" "false"
    rm -rf ${backupName}_*
    if [[ ${SKIP_COLLECT} == false ]] ; then
        ${COMMAND_DIR}/bkutils -d ${backupName} &> /dev/null
    fi
    set -e
}

collect_logs() {
    echo "Collecting logs.."
    local logsDir="${DIAGCOLLECT_OUTPUT}/logs"
    mkdir -p ${logsDir}

    set +e
    cd ${logsDir}
    mkdir info logs orders
    collect_data "/opt/storageos/logs" "${logsDir}/info" "true"
    convert_log_name
    zip -r logs-${DIAGCOLLECT_MAME}.zip * &>/dev/null
    rm -rf info logs orders
    set -e
}

collect_properties() {
    echo "Collecting properties.."
    local propDir="${DIAGCOLLECT_OUTPUT}/properties"
    mkdir -p ${propDir}
 
    set +e 
    /etc/systool --getprops > $propDir/systool-getprops
    /etc/systool --get-default > $propDir/systool-getdefault
    set -e
}

collect_health() {
    echo "Collecting health infomation.."
    local healthDir="${DIAGCOLLECT_OUTPUT}/health"
    mkdir -p ${healthDir}
    
    set +e
    echo -e -n "\tCollecting memory disk related information on local node.."
    free > ${healthDir}/${LOCAL_NODE}-memory 
    df -l > ${healthDir}/${LOCAL_NODE}-space 
    ifconfig > ${healthDir}/${LOCAL_NODE}-ifconfig 
    local task="/etc/diagtool -v > ${healthDir}/${LOCAL_NODE}-diagtool"
    execute_with_progress_point "${task}" "false"

    echo -e -n "\tCollecting nodetool realted information of each node.."
    for i in $(seq 1 ${NODE_COUNT})
    do
        local viprNode=$(get_nodeid)
        ${COMMAND_DIR}/nodetool -h ${viprNode} -p 7199 status &> ${healthDir}/${viprNode}-dbstatus  &
        ${COMMAND_DIR}/nodetool -h ${viprNode} -p 7299 status &> ${healthDir}/${viprNode}-geodbstatus &
        ${COMMAND_DIR}/nodetool -h ${viprNode} compactionhistory &> ${healthDir}/${viprNode}-compactionhistory &
        ${COMMAND_DIR}/nodetool -h ${viprNode} compactionstats &> ${healthDir}/${viprNode}-compactionstats &
        ${COMMAND_DIR}/nodetool -h ${viprNode} cfstats &> ${healthDir}/${viprNode}-cfstats &
        print_progress_point
    done
    wait
    echo "finish"
    set -e
}

collect_all() {
    echo "We are collecting a complete set of diagnostic data.."
    collect_logs
    collect_properties
    collect_health
    collect_backup
    collect_zk
    collect_cfs "-all_cfs"
}

collect_quick() {
    echo "We are collecting a default set of diagnostic data..."
    collect_logs
    collect_properties
    collect_health
    collect_zk
    collect_cfs "-min_cfs"
}

create_archive() {
    echo -n "Creating the final archive(${DIAGCOLLECT_DIR}/${DIAGCOLLECT_ARCHIVE_NAME}.zip).."
    cd ${DIAGCOLLECT_DIR}

    set +e
    local task="zip -r ${DIAGCOLLECT_ARCHIVE_NAME}.zip ${DIAGCOLLECT_MAME}/*"
    execute_with_progress_point "${task}" "true"
    local result=$?
    set -e
    if [ ${result} -eq 0 ]; then
        rm -rf ${DIAGCOLLECT_OUTPUT}
    else
        echo "Exit."
        return ${result}
    fi
}

upload_to_ftp() {
    local ftpserver=$1
    local user=$2
    local password=$3

    cd ${DIAGCOLLECT_DIR}
    local uploadfile=${DIAGCOLLECT_ARCHIVE_NAME}.zip
    echo -n "Uploading ${uploadfile} to ftp server(${ftpserver}).."

    set +e
    local task="cat ${uploadfile} | curl -sSk -u ${user}:${password} -a -T - "${ftpserver}"/${DIAGCOLLECT_ARCHIVE_NAME}.zip"
    execute_with_progress_point "${task}" "true"
    local result=$?
    set -e
    if [ $? -eq 0 ]; then
        echo "Removing local archive file.."
        rm -rf ${uploadfile}
    else 
        echo "Exit."
        return ${result}
    fi
}

######################
# Script Libs
######################
collect_data() {
    local sourceDir=${1}
    local targetDir=${2}
    local haveSubDir=${3}
    
    for i in $(seq 1 ${NODE_COUNT})
    do
        local viprNode=$(get_nodeid)
        if [ $haveSubDir == "true" ]; then
            local targetDir="${2}/$viprNode"
            mkdir -p $targetDir
        fi
        scp -r svcuser@"$viprNode":"${sourceDir}"/* ${targetDir} &>/dev/null
    done
    wait
}

get_nodeid() {
    if [ ${NODE_COUNT} -eq 1 ]; then
        echo "${LOCAL_NODE}"
    else
        echo "vipr$i"
    fi
}

get_nodeName() {
    echo `/etc/systool --getprops | grep "node_${i}_name" | awk -F '=' '{print $2}'`
}

user_confirm() {
    local message=${1}
    while true; do
        read -p "$message(yes/no)" yn
        case $yn in
            [Yy]es ) break;;
            [Nn]o )  echo "Exiting.."; exit;;
            * ) echo "Invalid input.";;
        esac
    done
}

execute_with_progress_point() {
    local task=$1
    local printResult=$2

    while true; do echo -n "."; sleep 5; done &
    BACKGROUND_PRINT_PID=$!
    local result="success"
    eval "${task}" &>/dev/null
    if [ $? -ne 0 ]; then
        result="failed"
    fi
    kill_backup_ground_thread &> /dev/null

    if [[ ${printResult} == "true" ]]; then
        echo "${result}"
    else
        echo "finish"
    fi
    if [[ ${result} == "failed" ]]; then
        return 1
    fi
}

kill_backup_ground_thread() {
    while true; do
        ps -p ${BACKGROUND_PRINT_PID} | grep -v "PID" &>/dev/null
        if [ $? -eq 0 ]; then
            kill -9  ${BACKGROUND_PRINT_PID} &>/dev/null
        else 
            break
        fi
    done
}

set_max_current_job_number() {
    # Number of CPU cores (default to 4)
    cat /proc/cpuinfo | grep processor &>/dev/null
    if [ $? -eq 0 ]; then
        MAX_CURR_JOBS=`cat /proc/cpuinfo | grep processor | wc -l`
    else
        MAX_CURR_JOBS=4
    fi
}

print_progress_point() {
    while [ `jobs -r | grep "Running" | wc -l` -ge $MAX_CURR_JOBS ]; do
        echo -n "." 
        sleep 5
    done
}

check_if_skip() {
    SKIP_COLLECT=false

    local param=$1
    if [[ ${NODE_AVAILABLE} == false ]]; then
        echo "Local node is unavailable, can not collect ${param}.."
        SKIP_COLLECT=true
        return
    fi
    if [[ ${CLUSTER_AVAILABLE} == false ]]; then
        echo "Cluster is unavailable, can not collect ${param}.."
        SKIP_COLLECT=true
    fi
}

check_node_cluster_status() {
    NODE_AVAILABLE=false
    CLUSTER_AVAILABLE=false

    for i in $(seq 1 ${NODE_COUNT})
    do
        local viprNode=$(get_nodeid)
        local tmpFile="/tmp/zk_telnet_status"
        echo ruok | curl telnet://${viprNode}:2181 &> ${tmpFile}
        cat ${tmpFile} | grep "imok" &>/dev/null
        if [ $? -eq 0 ]; then
            if [ ${viprNode} == ${LOCAL_NODE} ]; then
                NODE_AVAILABLE=true
            fi
            echo stat | curl telnet://${viprNode}:2181 &>${tmpFile}
            cat ${tmpFile} | grep "Mode" &>/dev/null
            if [ $? -eq 0 ]; then
                CLUSTER_AVAILABLE=true
            fi
        fi
        if [[ $NODE_AVAILABLE == true && $CLUSTER_AVAILABLE == true ]]; then
            break;
        fi
    done

    if [[ ${NODE_AVAILABLE} == false ]] ;then
        echo "Key service(s) on this node is unavailable, some collection would be missed, we'd better change to execute the tool on a healthy node."
        user_confirm "Are you sure you want to continue?"
    fi

    if [[ ${CLUSTER_AVAILABLE} == false ]]; then
        echo "Cluster is unavailable, some collection would be missed"
    fi
}

diagnose_cfs() {
    local cfsDir=$1
    local option=$2

    # Get to be connected CFs list
    local cfList=()
    if [ "${option}" == "-min_cfs" ]; then
        cfList=(${MIN_CFS[*]})
    elif [ "${option}" == "-all_cfs" ]; then
        cfList=$($COMMAND_DIR/dbcli show_cf | cut -d " " -f3)
    else
        echo "Invalid option for diagnosing cfs: ${option}"
        exit 2
    fi

    local output="lists"
    local cfListFile="collected_cf_list"
    rm -rf ${cfsDir}
    mkdir -p ${cfsDir}/${output}
    echo "${cfList}" > ${cfsDir}/${cfListFile}

    # Generate the "dbutils list" output files
    # processed in parallel with:  no. of threads == no. of CPU cores
    echo -n -e "\tNow generating dbutils listings, this may take several minutes..."
    for CF in ${cfList[@]}; do
        nohup $COMMAND_DIR/dbutils list ${CF} > ${cfsDir}/${output}/"dbutils.list.${CF}.out" 2> ${cfsDir}/${output}/cfs_dump.err < /dev/null &
        print_progress_point
    done
    wait
    echo "finish"

    # Format the generated "dbutils list" output files
    # (further formatting can be added with additional "else if" clauses in the "gawk" program below)
    for outfile in `ls ${cfsDir}/${output}/*.out` ;
    do
        gawk '(NR>2) { 
            if ((match($0, /(.+)time=(.+)/, a)) != 0) {
                match(a[2], /(^[0-9]+)(.+)/, b)
                strlength = length(b[1]) 
                epochtime1 = substr(b[1], 1, (strlength-3))
                millisecs1 = substr(b[1], (strlength-2), 3)
                print(a[1] "\n                time=" epochtime1 millisecs1 "\n                " strftime("%Y-%m-%d %H:%M:%S",epochtime1) " " millisecs1 "ms UTC\n" b[2])
            }
            else if ( (match($0,/(.+)Time = (.+)/,a) != 0) && (a[1] !~ "creation") && (a[2] !~ /^0/) )    {
                strlength = length(a[2]) 
                epochtime1 = substr(a[2], 1, (strlength-3))
                millisecs1 = substr(a[2], (strlength-2), 3)
                print(a[1] "Time = " epochtime1 millisecs1 " (" strftime("%Y-%m-%d %H:%M:%S",epochtime1) " " millisecs1 "ms UTC)")
            } 
            else if ($1 == "id:")           {
                print "\n * * * * * * * * *  * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n\n",$0;
            }
            else if ($0 ~ /^Number of All Records is:/)             {
                print "\n * * * * * * * * *  * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n\n",$0;
            }
            else {print $0}
        }' $outfile > "${cfsDir}/${output}/`basename $outfile .out`.txt"
    done

    # Generate cqlsh RelationIndex and AltIndex output
    echo -e -n "\tCapturing RelationIndex output, AltIndex output and system properties..."
    local indexList=(RelationIndex AltIdIndex)
    for index in ${indexList[@]}
    do
        local tmpFile="${cfsDir}/cqlsh.Input"
        echo "SELECT * FROM \"$index\" LIMIT 1000000;" >${tmpFile}
        $COMMAND_DIR/cqlsh -k StorageOS localhost -f ${tmpFile} > ${cfsDir}/${output}/"cqlsh.${index}_Output.txt"
    done
    echo "finish"
    rm -rf ${tmpFile}

    # Clean up temporary files and zip up the output files
    echo -e -n "\tArchiving cfs data.."
    cd ${cfsDir}/${output}
    tar -cf "unformatted_output.tar" *.out --format=gnu &>/dev/null
    rm -f *.out
    cd ${cfsDir}
    tar -czf "cfs-`date +%Y%0m%0d%0k%0M%0S`.tar.gz" ${output}/* --format=gnu &>/dev/null
    rm -rf ${output}
    echo "finish"
}

convert_log_name() {
    local serviceList=(vasasvc portalsvc systemevents coordinatorsvc apisvc bkutils geosvc dbsvc authsvc controllersvc syssvc sasvc geodbsvc)
    for service in ${serviceList[@]}; do
        for i in $(seq 1 ${NODE_COUNT})
        do
            local nodeId=$(get_nodeid)
            local nodeName=$(get_nodeName)
            local serviceLogFiles=($(ls -f info/${nodeId}/${service}.log.* 2>/dev/null))
            for file in ${serviceLogFiles[@]}; do
                zcat ${file} >> info/${nodeId}/${service}.log
            done
            if [ -f "info/${nodeId}/${service}.log" ]; then
                sed "s/^/${nodeId} ${nodeName} ${service} &/g" info/${nodeId}/${service}.log > logs/${service}_${nodeId}_${nodeName}.log
            fi
            rm -rf info/${nodeId}/${service}.log*
        done
    done
}

clean_up() {
    kill_backup_ground_thread
}

#######################
# Validate Parameters
#######################
genaral_param_count=0
plain_param_count=0
ftp_param_count=0
cf_param_count=0

check_ftp_parameter() {
    local param=$1
    local next_param=$2

    if [[ ! -n "${next_param}" ]] || [[ "${next_param}" == -* ]]; then
        echo "Invalid value of '$param'"
        usage
        exit 2
    fi
}

set_backupname() {
    BACKUP_NAME=$1
    if [[ ! -n "${BACKUP_NAME}" ]] || [[ "${BACKUP_NAME}" == -* ]]; then
        BACKUP_NAME="${DIAGCOLLECT_MAME}"
    fi
}

validate_parameters() {
    if [[ ${genaral_param_count} -eq 0 ]] && [[ ${plain_param_count} -eq 0 ]]; then
        echo "Lack of mandatory paramter"
        usage
        exit 2
    fi

    if [[ ${genaral_param_count} -gt 1 ]] || [[ (${genaral_param_count} -ne 0) && (${plain_param_count} -ne 0) ]]; then
        echo "'-all/quick' could not be executed together with other commands"
        usage
        exit 2
    fi

    if [[ ${cf_param_count} -gt 1 ]]; then
        echo "'-min_cfs' and '-all_cfs' could not be executed together"
        usage
        exit 2
    fi

    if [[ ${ftp_param_count} -ne 0 && ${ftp_param_count} -ne 3 ]]; then
        echo "Lack of parameters for ftp server.."
        usage
        exit 2
    fi
}

if [ $# -eq 0 ]; then
    usage
    exit 2
fi

if [ "$1" == "--help" -o "$1" == "-h" -o "$1" == "-help" ]; then
    usage
    exit 0
fi

for i in $(seq 1 $#)
do
    eval param=\${${i}}
    case $param in
        -all|-quick)
            genaral_param_count=$[genaral_param_count+1]
            ;;
        -min_cfs|-all_cfs)
            plain_param_count=$[plain_param_count+1]
            cf_param_count=$[cf_param_count+1]
            ;;
        -zk|-logs|-properties|-health)
            plain_param_count=$[plain_param_count+1]
            ;;
        -backup)
            plain_param_count=$[plain_param_count+1]
            if [[ $i -lt $# ]]; then
                eval next_param=\${$[i+1]}
            fi
            set_backupname "${next_param}"
            ;;
        -ftp)
            ftp_param_count=$[ftp_param_count+1]
            if [[ $i -lt $# ]]; then
                eval FTP=\${$[i+1]}
            fi
            check_ftp_parameter "$param" "$FTP"
            ;;
        -u)
            ftp_param_count=$[ftp_param_count+1]
            if [[ $i -lt $# ]]; then
                eval USER=\${$[i+1]}
            fi
            check_ftp_parameter "$param" "$USER"
            ;;
        -p)
            ftp_param_count=$[ftp_param_count+1]
            if [[ $i -lt $# ]]; then
                eval PASSWORD=\${$[i+1]}
            fi
            check_ftp_parameter "$param" "$PASSWORD"
            ;;
        -*)
            echo "Invalid Paramter: $param"
            usage
            exit 2;;
    esac
done

validate_parameters

###################
# Diag Collect Begin
###################
NODE_COUNT=`/etc/systool --getprops | awk -F '=' '/\<node_count\>/ {print $2}'`
LOCAL_NODE=`/etc/systool --getprops | awk -F '=' '/\<node_id\>/ {print $2}'`

check_node_cluster_status
set_max_current_job_number

trap clean_up EXIT

for i in $(seq 1 $#)
do
    eval param=\${$i}
    case ${param} in
        -all) collect_all; DIAGCOLLECT_ARCHIVE_NAME="${DIAGCOLLECT_ARCHIVE_NAME}${param}" ;;
        -quick) collect_quick; DIAGCOLLECT_ARCHIVE_NAME="${DIAGCOLLECT_ARCHIVE_NAME}${param}" ;;
        -min_cfs) collect_cfs ${param} ;;
        -all_cfs) collect_cfs ${param} ;;
        -zk) collect_zk ;;
        -backup) collect_backup "${BACKUP_NAME}" ;;
        -logs) collect_logs ;;
        -properties) collect_properties ;;
        -health) collect_health ;;
    esac
done

create_archive 2>/dev/null

if [[ ${ftp_param_count} -eq 3 ]]; then
    upload_to_ftp "${FTP}" "${USER}" "${PASSWORD}" 2>/dev/null
fi

echo "ViPR diag finished."
